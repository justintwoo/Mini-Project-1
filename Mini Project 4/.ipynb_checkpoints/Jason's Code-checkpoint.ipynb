{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# -- Sheet --\n",
    "\n",
    "import pandas as pd \n",
    "import re\n",
    "import os\n",
    "import string\n",
    "import csv\n",
    "import sys\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotADirectoryError",
     "evalue": "[Errno 20] Not a directory: 'searchTweets.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a9b8e90c400a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtweet_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"searchTweets.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"searchTweets.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtweet_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweet_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ticker\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"created\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sent_score\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: 'searchTweets.csv'"
     ]
    }
   ],
   "source": [
    "tweet_data2 = pd.read_csv(\"sentimenttweets.csv\")\n",
    "\n",
    "\n",
    "tweet_data = pd.concat([pd.read_excel(os.path.join(\"twitterdata\", f), index_col = 0) for f in os.listdir(\"twitterdata\")])\n",
    "\n",
    "tweet_data = tweet_data[[\"ticker\", \"created\", \"sent_score\"]]\n",
    "\n",
    "tweet_data2 = tweet_data2[[\"ticker\", \"created\", \"sent_score\"]]\n",
    "\n",
    "tweet_data = pd.concat([tweet_data, tweet_data2], axis = 0)\n",
    "\n",
    "tweet_data.shape\n",
    "\n",
    "returns_data = pd.read_excel(\"Returns_1120_1127.xlsx\", index_col = (0, 1))\n",
    "\n",
    "returns_data.head()\n",
    "\n",
    "returns_data.reset_index(inplace = True)\n",
    "\n",
    "returns_data[\"Date\"] = pd.to_datetime(returns_data[\"Date\"]).dt.strftime(\"%m-%d-%Y\")\n",
    "\n",
    "returns_data.rename(columns = {\"level_0\": \"ticker\", \"Date\": \"date\"}, inplace = True)\n",
    "\n",
    "returns_data.set_index([\"ticker\", \"date\"], inplace = True, drop = True)\n",
    "\n",
    "returns_data = returns_data[[\"Volume\", \"Adj Close\"]]\n",
    "\n",
    "returns = returns_data[\"Adj Close\"].groupby(level = 0).apply(lambda x: x / x[1:4].max())\n",
    "vol = returns_data.groupby(level = 0)[\"Volume\"].apply(lambda x: x / x[1:4].max())\n",
    "\n",
    "returns_data = pd.concat([returns, vol], axis = 1)\n",
    "returns_data.rename(columns = {\"Adj Close\": \"return\", \"Volume\": \"vol\"}, inplace = True)\n",
    "returns_data.dropna(inplace = True)\n",
    "\n",
    "tweet_data.rename(columns = {\"created\": \"date\"}, inplace = True)\n",
    "tweet_data.set_index([\"ticker\", \"date\"], inplace = True, drop = True)\n",
    "\n",
    "# Drop any tweets with 0 sentiment (irrelevant/couldn't be parsed/etc.)\n",
    "\n",
    "tweet_data = tweet_data[tweet_data[\"sent_score\"] != 0]\n",
    "\n",
    "# Calculate average sentiment score by date\n",
    "\n",
    "tweet_sent_data = tweet_data.groupby([\"ticker\", \"date\"])[\"sent_score\"].agg(['mean', 'count'])\n",
    "tweet_sent_data.columns = [\"sent_score\", \"tweet_volume\"]\n",
    "tweet_sent_data[\"tweet_volume\"] = tweet_sent_data.groupby(level = 0)[\"tweet_volume\"].apply(lambda x: x / x[1:4].max())\n",
    "\n",
    "combined_data = returns_data.merge(tweet_sent_data, left_index = True, right_index = True, how = \"left\")\n",
    "\n",
    "# In case of companies that didn't have any Twitter data for a particular date... \n",
    "combined_data.fillna(0, inplace = True)\n",
    "\n",
    "dt = combined_data.reset_index()\n",
    "\n",
    "dt[\"date\"] = pd.to_datetime(dt[\"date\"])\n",
    "dt.set_index([\"ticker\", \"date\"], inplace = True, drop = True)\n",
    "dt.sort_index(level = 1, ascending = False, inplace = True)\n",
    "\n",
    "target = dt[dt.index.get_level_values(1) == '11-27-2020']\n",
    "features = dt[dt.index.get_level_values(1) != '11-27-2020']\n",
    "\n",
    "target = target[\"return\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import random\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "train_tickers = list(random.sample(list(dt.index.get_level_values(0).unique()), 400))\n",
    "test_tickers = [x for x in dt.index.get_level_values(0).unique() if x not in train_tickers]\n",
    "\n",
    "import numpy as np \n",
    "\n",
    "X_train = np.array([features[features.index.get_level_values(0) == ticker].values.tolist() for ticker in train_tickers])\n",
    "y_train = np.array([target[target.index.get_level_values(0) == ticker].values.tolist() for ticker in train_tickers])\n",
    "\n",
    "X_test = np.array([features[features.index.get_level_values(0) == ticker].values.tolist() for ticker in test_tickers])\n",
    "y_test = np.array([target[target.index.get_level_values(0) == ticker].values.tolist() for ticker in test_tickers])\n",
    "\n",
    "X_test[1]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(units = 10, return_sequences = True, input_shape = (X_train.shape[1], X_train.shape[2])))\n",
    "model.add(LSTM(units = 5))\n",
    "model.add(Dense(units = 1))\n",
    "\n",
    "early_stop = EarlyStopping(monitor='loss', patience=10, verbose=1)\n",
    "\n",
    "model.compile(optimizer = \"adam\", loss = \"mean_squared_error\")\n",
    "\n",
    "model.fit(X_train, y_train, epochs = 50, batch_size = 30, callbacks=[early_stop])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "y_test_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(y_test, y_test_pred)\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "\n",
    "preds = pd.DataFrame({\"Predicted\": y_test_pred.flatten(), \"Actual\": y_test.flatten()})\n",
    "preds.plot()\n",
    "\n",
    "results = pd.DataFrame({\"Predicted\": y_test_pred.flatten(), \"Actual\": y_test.flatten()}, index = test_tickers)\n",
    "\n",
    "results.head()\n",
    "\n",
    "t_0 = features[features.index.get_level_values(1) == '11/25/2020'][[\"return\", \"sent_score\"]].reset_index(level = 1)\n",
    "\n",
    "res = results.merge(t_0, how = \"left\", left_index = True, right_index = True)\n",
    "\n",
    "res.rename(columns = {\"return\": \"t-1\"}, inplace = True)\n",
    "res.drop(columns = [\"date\"], inplace = True)\n",
    "\n",
    "res.head()\n",
    "\n",
    "ups = res[res[\"Predicted\"] > res[\"t-1\"]]\n",
    "\n",
    "downs = res[res[\"Predicted\"] < res[\"t-1\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Ups: {} Downs: {}\".format(ups.shape[0], downs.shape[0]))\n",
    "\n",
    "print(\"Out of {} predicted ups, {} actually went up\".format(ups.shape[0], res[(res[\"Predicted\"] > res[\"t-1\"]) & (res[\"Actual\"] > res[\"t-1\"])].shape[0]))\n",
    "\n",
    "print(\"Out of {} predicted downs, {} actually went down\".format(downs.shape[0], res[(res[\"Predicted\"] < res[\"t-1\"]) & (res[\"Actual\"] < res[\"t-1\"])].shape[0]))\n",
    "\n",
    "abs_up_ret = (ups[\"Actual\"] - ups[\"t-1\"]).sum()\n",
    "print(\"If we bought equally-weighted shares of each predicted up, we would have made: {:.2f}\".format(abs_up_ret))\n",
    "print(\"Of a total investment of {:.2f}, that's a 1-day return of {:.2f}%\".format(ups[\"t-1\"].sum(), abs_up_ret / ups[\"t-1\"].sum() * 100))\n",
    "print(\"Annualized, that figure is {:.2f}%\".format((np.power(1 + abs_up_ret / ups[\"t-1\"].sum(), 251) - 1) * 100))\n",
    "\n",
    "abs_down_ret = (downs[\"t-1\"] - downs[\"Actual\"]).sum()\n",
    "print(\"If we shorted equally-weighted shares of each predicted down, we would have made: {:.2f}\".format(abs_down_ret))\n",
    "print(\"Of a total investment of {:.2f} (@150% margin), that's a 1-day return of {:.2f}%\".format(downs[\"t-1\"].sum() * 1.5, abs_down_ret / (downs[\"t-1\"].sum() * 1.5) * 100))\n",
    "print(\"Annualized, that figure is {:.2f}%\".format((np.power(1 + abs_down_ret / (downs[\"t-1\"].sum() * 1.5), 251) - 1) * 100))\n",
    "\n",
    "# Impact of sentiment on performance\n",
    "\n",
    "top_sent = res.nlargest(20, columns = \"sent_score\")\n",
    "low_sent = res.nsmallest(20, columns = \"sent_score\")\n",
    "\n",
    "print(\"Of 20 companies with highest sentiment at t-1, {} went up, {} went down\".format(top_sent[top_sent[\"Actual\"] > top_sent[\"t-1\"]].shape[0], top_sent[top_sent[\"Actual\"] < top_sent[\"t-1\"]].shape[0]))\n",
    "\n",
    "print(\"Equal-weighted price return (long highest sentiment companies): {:.2f}\".format((top_sent[\"Actual\"] - top_sent[\"t-1\"]).sum()))\n",
    "\n",
    "print(\"Of 20 companies with lowest sentiment at t-1, {} went up, {} went down\".format(low_sent[low_sent[\"Actual\"] > low_sent[\"t-1\"]].shape[0], low_sent[low_sent[\"Actual\"] < low_sent[\"t-1\"]].shape[0]))\n",
    "\n",
    "print(\"Equal-weighted price return (short lowest sentiment companies): {:.2f}\".format((low_sent[\"t-1\"] - low_sent[\"Actual\"]).sum()))\n",
    "\n",
    "tr = (top_sent[\"Actual\"] - top_sent[\"t-1\"]).sum() + (low_sent[\"t-1\"] - low_sent[\"Actual\"]).sum()\n",
    "print(\"Strategy: Long Top 20 Sentiment, Short Worst 20 Sentiment\")\n",
    "print(\"Net price return: {:.2f}\".format(tr))\n",
    "print(\"Net 1-day : {:.2f}%\\nAnnualized: {:.2f}%\".format(100 * tr / (top_sent[\"t-1\"].sum() + 1.5 * low_sent[\"t-1\"].sum()), (np.power(1 + tr / (top_sent[\"t-1\"].sum() + 1.5 * low_sent[\"t-1\"].sum()), 251) - 1) * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
